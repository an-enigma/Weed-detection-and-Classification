{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "os.listdir('../input/v2-plant-seedlings-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import imageio\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get total images in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = os.listdir('../input/v2-plant-seedlings-dataset')\n",
    "total_images = 0\n",
    "# loop through each folder\n",
    "for folder in folder_list:\n",
    "    # set the path to a folder\n",
    "    path = '../input/v2-plant-seedlings-dataset/' + str(folder)\n",
    "    # get a list of images in that folder\n",
    "    images_list = os.listdir(path)\n",
    "    # get the length of the list\n",
    "    num_images = len(images_list)\n",
    "    \n",
    "    total_images = total_images + num_images\n",
    "    # print the result\n",
    "    print(str(folder) + ':' + ' ' + str(num_images))\n",
    "\n",
    "# print the total number of images available\n",
    "print('Total Images: ', total_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling all folders into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    all_images_dir = 'images_all'\n",
    "    os.mkdir(all_images_dir)\n",
    "except:\n",
    "    print(\"Already there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = os.listdir('../input/v2-plant-seedlings-dataset')\n",
    "for folder in folder_list:\n",
    "    #path to the folder\n",
    "    if(folder=='nonsegmentedv2'):\n",
    "        continue\n",
    "    path = '../input/v2-plant-seedlings-dataset/' + str(folder)\n",
    "    #list of all files in the folder\n",
    "    file_list = os.listdir(path)\n",
    "    # move the 0 images to images_all\n",
    "    for fname in file_list:\n",
    "        # source path to image\n",
    "        src = os.path.join(path, fname)\n",
    "        # Change the file name because many images have the same file name.\n",
    "        # Add the folder name to the existing file name.\n",
    "        new_fname = str(folder) + '_' + fname\n",
    "        # destination path to image\n",
    "        dst = os.path.join(all_images_dir, new_fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir('images_all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = os.listdir('images_all')\n",
    "#dataframe of all images\n",
    "df_data = pd.DataFrame(image_list, columns=['image_id'])\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the class names from the file names of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(x):\n",
    "    # split into a list\n",
    "    a = x.split('_')\n",
    "    # the target is the first index in the list\n",
    "    cname = a[0]\n",
    "    return cname\n",
    "df_data['target'] = df_data['image_id'].apply(get_class)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to balance the dataset for better classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE=250\n",
    "IMAGE_SIZE = 128\n",
    "target_list = os.listdir('../input/v2-plant-seedlings-dataset')\n",
    "i=target_list.index(\"nonsegmentedv2\")\n",
    "target_list.pop(i)\n",
    "#print(target_list)\n",
    "for target in target_list:\n",
    "    # Filter out a target and take a random sample\n",
    "    df = df_data[df_data['target'] == target].sample(SAMPLE_SIZE, random_state=101)\n",
    "    # if it's the first item in the list\n",
    "    if target == target_list[0]:\n",
    "        df_sample = df\n",
    "    else:\n",
    "        # Concat the dataframes\n",
    "        df_sample = pd.concat([df_sample, df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_sample['target']\n",
    "df_train, df_val = train_test_split(df_sample, test_size=0.10, random_state=101, stratify=y)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validation folders created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    base_dir = 'base_dir'\n",
    "    os.mkdir(base_dir)\n",
    "    train_dir = os.path.join(base_dir, 'train_dir')\n",
    "    os.mkdir(train_dir)\n",
    "    val_dir = os.path.join(base_dir, 'val_dir')\n",
    "    os.mkdir(val_dir)\n",
    "\n",
    "\n",
    "\n",
    "    for folder in folder_list:\n",
    "        folder = os.path.join(train_dir, str(folder))\n",
    "        os.mkdir(folder)\n",
    "    for folder in folder_list:\n",
    "        folder = os.path.join(val_dir, str(folder))\n",
    "        os.mkdir(folder)\n",
    "except:\n",
    "    print(\"already created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('base_dir/train_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.set_index('image_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize and save images to respective directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = list(df_train['image_id'])\n",
    "val_list = list(df_val['image_id'])\n",
    "for image in train_list:\n",
    "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
    "    fname = image\n",
    "    # get the label for a certain image\n",
    "    folder = df_data.loc[image,'target']\n",
    "    # source path to image\n",
    "    src = os.path.join(all_images_dir, fname)\n",
    "    # destination path to image\n",
    "    dst = os.path.join(train_dir, folder, fname)\n",
    "    \n",
    "    # resize the image and save it at the new location\n",
    "    image = cv2.imread(src)\n",
    "    image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    # save the image at the destination\n",
    "    cv2.imwrite(dst, image)\n",
    "\n",
    "for image in val_list:\n",
    "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
    "    fname = image\n",
    "    # get the label for a certain image\n",
    "    folder = df_data.loc[image,'target']\n",
    "    # source path to image\n",
    "    src = os.path.join(all_images_dir, fname)\n",
    "    # destination path to image\n",
    "    dst = os.path.join(val_dir, folder, fname)\n",
    "    # resize the image and save it at the new location\n",
    "    image = cv2.imread(src)\n",
    "    image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    # save the image at the destination\n",
    "    cv2.imwrite(dst, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross check the train data count for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = os.listdir('base_dir/train_dir')\n",
    "total_images = 0\n",
    "# loop through each folder\n",
    "for folder in folder_list:\n",
    "    # set the path to a folder\n",
    "    path = 'base_dir/train_dir/' + str(folder)\n",
    "    # get a list of images in that folder\n",
    "    images_list = os.listdir(path)\n",
    "    # get the length of the list\n",
    "    num_images = len(images_list)\n",
    "    total_images = total_images + num_images\n",
    "    # print the result\n",
    "    print(str(folder) + ':' + ' ' + str(num_images))\n",
    "# print the total number of images available\n",
    "print('Total Images: ', total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rmdir('base_dir/train_dir/nonsegmentedv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross check the validation data count for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = os.listdir('base_dir/val_dir')\n",
    "total_images = 0\n",
    "# loop through each folder\n",
    "for folder in folder_list:\n",
    "    # set the path to a folder\n",
    "    path = 'base_dir/val_dir/' + str(folder)\n",
    "    # get a list of images in that folder\n",
    "    images_list = os.listdir(path)\n",
    "    # get the length of the list\n",
    "    num_images = len(images_list)\n",
    "    total_images = total_images + num_images\n",
    "    # print the result\n",
    "    print(str(folder) + ':' + ' ' + str(num_images))\n",
    "# print the total number of images available\n",
    "print('Total Images: ', total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rmdir('base_dir/val_dir/nonsegmentedv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'base_dir/train_dir'\n",
    "valid_path = 'base_dir/val_dir'\n",
    "\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "train_batch_size = 10\n",
    "val_batch_size = 10\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=train_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "# Note: shuffle=False causes the test dataset to not be shuffled\n",
    "test_gen = datagen.flow_from_directory(valid_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step is to create the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = (3,3)\n",
    "pool_size= (2,2)\n",
    "first_filters = 32\n",
    "second_filters = 64\n",
    "third_filters = 128\n",
    "\n",
    "dropout_conv = 0.3\n",
    "dropout_dense = 0.3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu', \n",
    "                 input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size)) \n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(dropout_dense))\n",
    "model.add(Dense(12, activation = \"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.0001), loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, \n",
    "                                   verbose=1, mode='max', min_lr=0.00001)\n",
    "                              \n",
    "                              \n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=10, verbose=1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the validation loss and accuracy.\n",
    "\n",
    "# Here the best epoch will be used.\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "val_loss, val_acc = model.evaluate_generator(test_gen, steps=len(df_val))\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
